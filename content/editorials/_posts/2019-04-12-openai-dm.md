---
title: "How have DOTA and StarCraft wins helped progress of AI research?"
image:
  feature: content/editorials/images/openai-dm/oai-dm-rl.jpg
author: thomas_lux
editor: andrey_kurenkov
tags: [hype,rl,games]
excerpt: "Impressive results have been obtained by scaling known techniques, and the long-term prospects remain unclear"
permalink: /editorials/openai-dm
---
<figure>
  <img src="/content/editorials/images/openai-dm/game_comparison.jpg" alt="game_comparison"/>
  <figcaption>
A table showcasing why StarCraft and Dota II are far more challenging than challenges already tackled by AI. <a href="https://forums.spacebattles.com/threads/deepmind-ai-starcraft-ii-demonstration-alphastar-livestream.720202/">Source</a>.
  </figcaption>
</figure>

Recently we’ve seen much-hyped public demonstration of advances in reinforcement learning (RL)
with [OpenAI Five](https://openai.com/blog/openai-five/) and
DeepMind’s
[AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
competing against human gamers in DOTA II and StarCraft respectively,
and we’re set to see [another demonstration](https://openai.com/blog/openai-five-finals/) this
Saturday. It’s become a standard trend for research labs and companies
to broadcast such achievements with the aim of impressing the general
public (and likely investors) as much as contributing to the field of AI
research. So, it is important to distinguish the PR from the research
progress. And in particular, it is worth noting that both of these
achievements were made possible by some simplifications of their
respective domains and scaling-up versions of existing technology rather
than groundbreaking new ideas.

Of course OpenAI (OAI) and DeepMind (DM) definitely *are* steadily
producing promising new algorithmic ideas, as are other large industry
AI labs such as [Uber Engineering](https://eng.uber.com/go-explore/),
[NVIDIA](https://news.developer.nvidia.com/nvidia-researchers-develop-reinforcement-learning-algorithm-to-train-thousands-of-robots-simultaneously/),
and [Facebook AI](https://code.fb.com/ml-applications/horizon/). And,
scaling in itself is valuable for research as it shows what’s possible
and leads to new valuable insights. Let’s try to break down some of the
advances to see what they mean for the future of RL and what we can
realistically expect to see in the upcoming match.

OpenAI Five at the International
--------------------------------

<figure>
  <img src="/content/editorials/images/openai-dm/international.jpg" alt="international"/>
  <figcaption>
Image from The International DOTA II tournament.
  </figcaption>
</figure>

[The Open AI Five](https://openai.com/blog/openai-five/) experiment
captured a [lot](https://www.youtube.com/watch?v=UZHTNBMAfAA) of
[public](https://www.youtube.com/watch?v=eaBYhLttETw)
[attention](https://en.wikipedia.org/wiki/OpenAI_Five). First, there
was a [show match](https://www.youtube.com/watch?v=_QQYaVUODkE)
against an ad-hoc team of retired DotA II professional players. This
demo hyped up the public because the RL bots won every standard match,
suggesting the DotA II agents might beat real pro teams. Not long after,
OAI played off against top human professionals in a
[showcase](https://openai.com/blog/the-international-2018-results/) at
the International (a major DotA II annual event). The five RL bots
featured impressive team fighting and coordination, but eventually lost
each match against the professionals in the “late game” due to a lack of
long term strategy. These matches demonstrated some of the incredible
strengths and also the yet-unsolved
[limitations](https://www.reddit.com/r/MachineLearning/comments/99ix2d/d_openai_five_loses_against_first_professional/)
of reinforcement learning.

<iframe width="806" height="453"
src="https://www.youtube.com/embed/UZHTNBMAfAA"
frameborder="0" allow="accelerometer; autoplay; encrypted-media;
gyroscope; picture-in-picture" allowfullscreen></iframe>

The major achievements for this work are [summarized well](https://openai.com/blog/openai-five/#theproblem) by OAI: DOTA II
is a problem that is orders of magnitude more difficult than those
addressed in prior works due to the number of available actions, number
of observations, number of steps in a single game, and the amount of
hidden information that must be inferred. But as covered well in
Motherboard’s [“OpenAI Is Beating Humans at ‘Dota 2’ Because It’s Basically Cheating](https://motherboard.vice.com/en_us/article/gy3nvq/ai-beat-humans-at-dota-2)”,
there are several important caveats to be aware of. In order to achieve
these results, the OAI team used:

1.  **API and superhuman speed**: The bots were trained using API to get
    precise game state as thousands of exact numbers, rather than
    playing by looking at the pixels of the game as humans do. This,
    and a lack of other restrictions, allows the bots to have
    ‘superhuman’ accuracy and speed which makes them very hard to beat
    in head-to-head short term fights that don’t require much
    long-term strategy.

2.  **Simplifying some tasks during training**: OAI [modified properties of the game](https://twitter.com/catherineols/status/1026257950044696576) during training to encourage agents to explore important but possibly challenging strategic decisions (like killing Roshan).

3.  **Hand-designing the model**: The team also designed a complex neural net
    [architecture](https://s3-us-west-2.amazonaws.com/openai-assets/dota_benchmark_results/network_diagram_08_06_2018.pdf)
    specifically for the task.

4.  **Limitations on the game**: Especially for the first demonstrations
    but even for the final one, [multiple restrictions](https://openai.com/blog/openai-five-benchmark/#restrictions)
    were placed on standard gameplay—most notably, the roster of playable characters was reduced.

5.  **Reward shaping**: Perhaps most importantly, OAI made heavy use of
    [reward tuning](https://medium.com/@evanthebouncy/understanding-openai-five-16f8d177a957),
    a process where short term goals are explicitly rewarded instead
    of only focusing on winning the game. In simple terms, the OAI
    team directly encouraged its bots towards learning
    certain behaviors. This is in contrast to something like
    [AlphaZero](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)
    that learns its strategies solely from self play and match
    outcome, which is a much less informative, *sparse* reward
    for training.

So, while an impressive achievement, it should be noted it was achieved
partially by limiting the game and hard-coding human knowledge in the
learning process. And as we will discuss soon, their approach to the
problem rested on investing heavily in the same basic techniques used
for much simpler problems, rather than trying to invent new techniques
more appropriate for this challenging a task.

DeepMind AlphaStar and Team Liquid
----------------------------------

<figure>
  <img src="/content/editorials/images/openai-dm/alphastar.gif" alt="alphastar"/>
  <img src="/content/editorials/images/openai-dm/alphastar.png" alt="alphastar"/>
  <figcaption>
A visualization of the AlphaStar Starcraft bot playing and a photo of its developers viewing the match.
  </figcaption>
</figure>

Starcraft II presents another great challenge for reinforcement
learning, for largely the same reasons as DOTA II. Earlier this year DM
produced a [beautiful demonstration](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
of RL research by testing its own set of bots [against human
professionals](https://www.youtube.com/watch?v=6EQAsrfUIyo&feature=youtu.be).
On a broadcasted [show](https://youtu.be/cUTMhmVh1qs?t=1330), the DM
bots beat a human professional 5-0, managing to win in situations in
which [professionals didn’t think](https://youtu.be/cUTMhmVh1qs?t=6198) winning was possible. But,
as with the OpenAI bots, DM’s prowess was achieved in part by imposing
significant limitations on the game, as covered well in “[An AI crushed two human pros at StarCraft—but it wasn’t a fair fight](https://arstechnica.com/gaming/2019/01/an-ai-crushed-two-human-pros-at-starcraft-but-it-wasnt-a-fair-fight/)”.
Most significantly:

1.  **API and superhuman speed:** Like OAI5, DM’s bots were trained with
    access to exact game state via the game’s API. Although DM did
    impose some limits on the speed at which the bots could issue
    actions, [analysis](https://blog.usejournal.com/an-analysis-on-how-deepminds-starcraft-2-ai-s-superhuman-speed-could-be-a-band-aid-fix-for-the-1702fb8344d6)
    of these limitations showed that they still allowed the bots to
    have the benefit of super-human speed, similarly to the OAI5 bots.

2.  **Full map observations**: the game was much simplified for DM due
    to it observing the entire game map rather than portions of it at
    a time as human players do, and when this simplification was taken
    away in the final match DM lost.

3.  **Hand-designing the model**: as with OAI5, the neural net used in
    this case was complicated and not general purpose but more
    specific to the task.

4.  **Limitations on the game**: The bots were also only demonstrated to
    work on one map, and with one of the three playable races
    (arguably, the easiest one to master).

<figure>
  <img src="/content/editorials/images/openai-dm/progress.gif" alt="progress"/>
  <figcaption>
Progress and change in the AlphaStar bots strategy over the course of training. From <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">the official blog post</a>.
  </figcaption>
</figure>

Both experiments point to an obvious imminent question, how do we solve
the problem of long term planning?

The Algorithms that Brought Us Here
-----------------------------------

Now we have agents that can play video games! So, is that it? Underneath
all this gaming is a guided attempt by researchers to tackle
environments with sparse rewards[^1], large action spaces[^2],
stochasticity[^3], imperfect information[^4], and the need for long term
planning. Any one of these is an incredibly difficult problem to solve,
let alone all of them simultaneously. In simple terms, the best output
of the collective research effort in reinforcement learning manifests
itself consistently in three steps:

-   Randomly choose actions to explore the environment.

-   Via some model, make an agent remember which actions are *better*,
    prefer those.

-   Handcraft the environment, reward, and model to optimize for
    desired outcome.

This may not sound that impressive to you, so let’s get our hands a
little dirtier with details. OAI uses an augmented control [policy
optimization](https://openai.com/blog/openai-baselines-ppo/) with
[memory](http://colah.github.io/posts/2015-08-Understanding-LSTMs/#lstm-networks)
to attack the DotA II game environment, while DM uses two
[types](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
of [memory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf),
two types of sequential [decision](https://arxiv.org/pdf/1708.04782.pdf)
[making](https://papers.nips.cc/paper/5866-pointer-networks.pdf), and
a [learned reward](https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf)
function estimator to make their agents competitive. Effectively, OAI
scaled up an algorithm they implemented and used for smaller problems
before, and DM combined several cutting edge ideas into a fairly
complicated system. Notice anything missing? No new ideas to deal with
abstractions, no reasoning, no semblance of what we understand as the
human ability to *strategize* and *plan*. OAI and DM instead bet on
combining and scaling up existing ideas to tackle these tasks -- what we
might call engineering.

Not to say these outcomes are not beneficial for AI research; it is good
to know how far present day techniques can be pushed and how they can be
combined. Still, present day RL research mostly focuses on short term
tasks (involving, say sequences of 100s of choices, and not many tens of
thousands) and so the bots prowess was indeed in short-term gameplay
rather than long term strategy. It is too early to say whether new ideas
to deal with strategizing are needed or scaling up is sufficient, but
it’s certainly a question to be aware of.

Near Term Hopes and Expectations
--------------------------------

For upcoming research in RL, any explicit (or implicit) bridge between
short-term and long-term planning could show new promise. It would also
be amazing to see new efforts geared towards abstract reasoning within
models and a more intuitive specification of objectives. Perhaps these
things are out-of-reach given current tools, but progress demands new
ideas and effort.

<figure>
  <img src="/content/editorials/images/openai-dm/openai-future.png" alt="openai-future"/>
  <figcaption>
A tweet from OpenAI from soon after The International.
  </figcaption>
</figure>

Regarding the upcoming display, it seems unlikely that OAI would
schedule another heavily publicized match against professional human
players if they expect to lose, they’ll [probably win](https://twitter.com/OpenAI/status/1037765547427954688) on April
13th. Whether the OAI team has changed the architecture, trained for
hundreds (or thousands) more game-years, or fine tuned the reward
functions to encourage better long term trajectories, we can expect that
this collection of RL agents will look better than the last.

In Summary
----------

All of these reinforcement learning achievements would be difficult to
believe a few years ago. The work being done by these research labs is
exciting, but not always because of the algorithms involved so much as
its scale, effort, and collective public attention. Public efforts such
as these inspire the community to put more effort and thought into the
underlying steady march towards broadly applicable artificial
intelligence. If we keep getting excited as a community and letting the
scientific process guide us to new and better algorithms we will get
there, but don’t start holding your breath yet.

[^1]: Infrequent feedback from the environment about what is *good* and
    *bad*.

[^2]: There are many actions to choose from with continuous value, a
    very hard problem.

[^3]: The observations and actions each have their own random noise,
    making prediction more difficult.

[^4]: The entirety of the environment cannot be observed at once,
    inferences must be made about what is happening.
