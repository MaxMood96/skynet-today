---
layout: post
image:
  feature: assets/img/briefs/gpt3/main.jpg
  credit: <a href="https://www.theguardian.com/commentisfree/2020/aug/01/gpt-3-an-ai-game-changer-or-an-environmental-disaster/">Getty Images via The Guardian</a>
title: "GPT-3: A Breakthrough, but not Coming for Your Job"
excerpt: "GPT-3 still has a range of limitations which must be noted before worrying that it will cost anyone their livelihood."
author: [sathvik_nair, daniel_bashir]
tags: [NLP,OpenAI,hype]
categories: [briefs]
permalink: /briefs/gpt3
sidebartoc: true
highlight: true
---

## Summary

* Recently, OpenAI released the largest language model to date, GPT-3,
    which is functionally similar to many of its predecessors and
    differentiated primarily by having 10x more parameters than the
    previous largest language model and being trained on a much bigger
    dataset.
* This large quantitative difference allows a trained GPT-3 to achieve
    qualitative improvements over its predecessors; unlike the latter,
    the trained GPT-3 model can perform many different tasks without
    training specifically for those tasks.
* It has met significant acclaim from both the press and
    technologists, who describe its myriad applications and also several
    key limitations.
* Although GPT-3 represents remarkable progress, its limitations have
    gotten less attention, and are summarized here.

## What Happened

On May 28, OpenAI released the paper "[Language Models are Few-Shot
Learners](https://arxiv.org/abs/2005.14165)," which [introduced
GPT-3](https://openai.com/blog/openai-api/), the largest[^1] language
model[^2] ever created. The 73-page paper describes how GPT-3 follows
recent trends on significantly advancing the state of the art in
language modeling. Broadly, on natural language processing (NLP)
benchmarks, GPT-3 achieves promising, and sometimes competitive,
results.

GPT-3 represents the increase in performance that comes from using a
larger model, and it also follows the immense increases in model and
data size that characterize recent developments in NLP.
The paper's core message however was less about its performance on benchmarks, and more about the discovery that due to its scale GPT-3 
is capable of solving NLP tasks that it has never before encountered
after seeing just one or a few examples of the task ('few-shot' learning). 
This is in contrast to what is typically done today, which is that models
are re-trained (or 'fine-tuned') on a larger amount of additional data in order to perform new tasks.

<figure>
 <img src="{{ site.imgpath }}/briefs/gpt3/fine-tuning.png" alt=" In-context learning vs. fine-tuning, from GPT-3 paper."/>
  <figcaption>
  This shows some examples of GPT-3's approach (right) compared to
  fine-tuning (left). When gradient updates are performed, this means that
  the model's internal representations are adjusted to account for the new
  data that has been observed. From the GPT-3 paper.
  </figcaption>
</figure>

Last year, OpenAI developed [GPT-2](https://openai.com/blog/better-language-models/), which was
able to generate long, coherent texts that, on first glance, are
[difficult to distinguish from human writing](https://www.skynettoday.com/editorials/humans-not-concentrating).
OpenAI notes that they "use the same model and architecture as GPT-2[^5], but the size of the network itself, as well as
the data that it was trained on[^3], is much larger than its predecessor --
GPT-3 has 175 billion parameters compared to GPT-2's 1.5 billion, and
was trained on 570 billion gigabytes of text, while GPT-2 was trained on 40. While just increasing the scale in this way was not in itself novel, the new model's ability to perform few-shot learning was a novel discovery, and was demonstrated in the paper
via a variery of traditional Natural Language Processing tasks:

<figure>
 <img src="{{ site.imgpath }}/briefs/gpt3/example-2.png" alt=" Target completion example, from GPT-3 paper."/>
</figure>

<figure>
 <img src="{{ site.imgpath }}/briefs/gpt3/example-3.png" alt=" Target completion example, from GPT-3 paper."/>
</figure>

<figure>
 <img src="{{ site.imgpath }}/briefs/gpt3/example-4.png" alt=" Target completion example, from GPT-3 paper."/>
 <figcaption>
 Some examples of GPT-3 answering prompts from the paper itself.
 </figcaption>
</figure>

Following the release of the paper, it was announced on June 11 that GPT-3 will be usable by third-party developers through [OpenAI's API](https://openai.com/blog/openai-api/), which is the company's first commercial product and is in closed beta.
Access to GPT-3's API ["is invitation only, and pricing is undecided."](https://www.theverge.com/2020/6/11/21287966/openai-commercial-product-text-generation-gpt-3-api-customers)
After the release of the API, [multiple impressive demonstrations](https://gptcrush.com/resources/?ref=producthunt) of
GPT-3's [potential uses](https://gpt3examples.com/#examples) (in addition to writing [short articles](https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-writing-almost-anything/), [blog posts](https://maraoz.com/2020/07/18/openai-gpt3/), and
[creative fiction](https://www.gwern.net/GPT-3) [text generation](https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-writing-almost-anything/)) led to a flood of discussions within the AI community and beyond. One of
the most noted examples the demonstration that it could be made to generate functional JavaScript
code given a normal english description of the result:

<figure>
<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">This is mind blowing.<br><br>With GPT-3, I built a layout generator where you just describe any layout you want, and it generates the JSX code for you.<br><br>W H A T <a href="https://t.co/w8JkrZO4lk">pic.twitter.com/w8JkrZO4lk</a></p>&mdash; Sharif Shameem (@sharifshameem) <a href="https://twitter.com/sharifshameem/status/1282676454690451457?ref_src=twsrc%5Etfw">July 13, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

Many similar demonstrations on Twitter soon followed: 

<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">Words → website ✨<br><br>A GPT-3 × Figma plugin that takes a URL and a description to mock up a website for you. <a href="https://t.co/UsJz0ClGA7">pic.twitter.com/UsJz0ClGA7</a></p>&mdash; Jordan Singer (@jsngr) <a href="https://twitter.com/jsngr/status/1287026808429383680?ref_src=twsrc%5Etfw">July 25, 2020</a></blockquote> 

<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">After many hours of retraining my brain to operate in this &quot;priming&quot; approach, I also now have a sick GPT-3 demo: English to LaTeX equations! I&#39;m simultaneously impressed by its coherence and amused by its brittleness -- watch me test the fundamental theorem of calculus.<br><br>cc <a href="https://twitter.com/gdb?ref_src=twsrc%5Etfw">@gdb</a> <a href="https://t.co/0dujGOKaYM">pic.twitter.com/0dujGOKaYM</a></p>&mdash; Shreya Shankar (@sh_reya) <a href="https://twitter.com/sh_reya/status/1284746918959239168?ref_src=twsrc%5Etfw">July 19, 2020</a></blockquote> 

<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">Since getting academic access, I’ve been thinking about GPT-3’s applications to grounded language understanding — e.g. for robotics and other embodied agents.<br><br>In doing so, I came up with a new demo: <br><br>Objects to Affordances: “what can I do with an object?”<br><br>cc <a href="https://twitter.com/gdb?ref_src=twsrc%5Etfw">@gdb</a> <a href="https://t.co/ptRXmy197P">pic.twitter.com/ptRXmy197P</a></p>&mdash; Siddharth Karamcheti (@siddkaramcheti) <a href="https://twitter.com/siddkaramcheti/status/1286168606896603136?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote> 

Which then inspired a large amount of discussion and interest.

## The Reactions

The press, experts in the field, and the general tech community had
widely differing opinions on both GPT-3's capabilities and its potential
implications when extensively adapted. Responses have ranged from
[heralding the future of human productivity](https://nesslabs.com/gpt-3-future-productivity) and
[fear over losing jobs](https://towardsdatascience.com/will-gpt-3-kill-coding-630e4518c04d)
to more measured consideration of GPT-3's capabilities and limitations.

Coverage from the press has increased since the demos came out:

* The [MIT Technology Review](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/)
    provided sources that display the various ways GPT-3 can generate
    human-like text, from generating React code to writing poetry, while
    saying it "can generate amazing human-like text on demand but won\'t
    bring us closer to true intelligence."
* The [Verge](https://www.theverge.com/2020/6/11/21287966/openai-commercial-product-text-generation-gpt-3-api-customers)
    focused on the potential commercial applications of GPT-3.
* In the wake of discussions on the topic, news sources such as
    [Forbes](https://www.forbes.com/sites/robtoews/2020/07/19/gpt-3-is-amazingand-overhyped/#149bdd951b1c)
    and
    [VentureBeat](https://venturebeat.com/2020/05/29/openai-debuts-gigantic-gpt-3-language-model-with-175-billion-parameters/)
    considered issues such as model bias and
    [hype](https://metro.co.uk/2020/07/17/artificial-intelligence-change-programming-know-13002280)
    in their coverage of GPT-3.
* In addition to [noting its imperfections](https://www.wired.com/story/ai-text-generator-gpt-3-learning-language-fitfully/),
    [Wired](https://www.wired.com/story/ai-generated-text-is-the-scariest-deepfake-of-all/)
    pointed out that GPT-3 could introduce an even scarier version
    of deepfake technology, generating media for which there is no
    unaltered original to compare it to or to fact-check against.
    Synthetic text in particular can be produced easily at large scale
    and has fewer tells to allow for detection.
* Similarly, an opinion piece from New York Times columnist Farhad
    Manjoo titled
    ["The new frontier in AI is amazing, promising \... and a little
    scary."](https://www.startribune.com/the-new-frontier-in-ai-is-amazing-promising-and-a-little-scary/571973852/?refresh=true)
    ponders the possibility of GPT-3 replacing the author and likewise
    covers reasons for concern.
* Lastly, John Naughton, professor of the public understanding of technology at the Open University, writing for [The Guardian](https://www.theguardian.com/commentisfree/2020/aug/01/gpt-3-an-ai-game-changer-or-an-environmental-disaster), sees GPT-3 as merely "an incremental improvement on its predecessors rather than a dramatic conceptual breakthrough." Naughton warns that if such incremental improvements in technology are to be driven by applying more and more compute, the environmental costs will be enormous.

Compared to some of the press, reactions from experts in machine learning and
NLP are largely more tempered and driven by curiosity, focusing on
issues with how GPT-3 will be deployed and questioning its ability to
truly understand language.

* Yoav Goldberg, NLP professor at Bar-Ilan University in Israel and
    research scientist at the Allen Institute for Artificial
    Intelligence [reported
    results](https://twitter.com/yoavgo/status/1284314641770971140)
    from GPT-3 on linguistically interesting prompts. With
    one-line descriptions of tasks, Goldberg found some impressive
    results. For instance, GPT-3 successfully wrote the plural forms of
    words in a [sentence completion task](https://twitter.com/yoavgo/status/1284336466630516741).These
    were both novel words like "wug" and words with irregular plural
    forms in English like "men." He also noted how GPT-3 converted
    statements to the [passive voice](https://twitter.com/yoavgo/status/1284377124846153728) and
    [questions](https://twitter.com/yoavgo/status/1284382227959296001),
    and even produced answers to [basic Python programming questions](https://twitter.com/yoavgo/status/1285243645516546048).
    However, there are cases where GPT-3 falls short. One such prompt is
    "what do dolphins and eagles have in common?" and GPT-3 responds,
    "Both are birds." Goldberg also prompts the model with a description
    of a [scene](https://twitter.com/yoavgo/status/1284372882525650944),
    and it fails to answer certain questions accurately. These are both
    examples of cases where it is trivial to answer these questions
    using programs written from rule-based models, but GPT-3 fails to
    provide a correct answer.
    <blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">Following several conversations, I want to clarify:<br><br>I am amazed by its ability to learn various patterns of operation from 1-3 examples and carry them correctly. This is new and *super* exciting.<br><br>It does not &quot;master all language&quot; or &quot;understands text&quot;.<br><br>It is not sentient. <a href="https://t.co/WEkxxkVIJm">https://t.co/WEkxxkVIJm</a></p>&mdash; (((ل()(ل() &#39;yoav)))) (@yoavgo) <a href="https://twitter.com/yoavgo/status/1284557072118550532?ref_src=twsrc%5Etfw">July 18, 2020</a></blockquote> 

* Anima Anandkumar, director of AI research at NVIDIA and professor
    of computing and mathematical sciences at Caltech critiqued the
    OpenAI team for not paying significant attention to
    [bias](https://twitter.com/AnimaAnandkumar/status/1271135883022884864)
    in the model, especially considering that GPT-2 exhibited [similar
    issues](https://twitter.com/AnimaAnandkumar/status/1271137176529416193).
    This was because it was trained on data from unmoderated sources
    like Reddit, and thus would "learn" biases that are evident in texts
    written by humans (although it seems that the model can be primed to
    [reduce gender
    bias](https://twitter.com/AndrewLBeam/status/1288191547666833409)).
    
* Jerome Pesenti, head of AI at Facebook, shared similar concerns:
<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/gpt3?src=hash&amp;ref_src=twsrc%5Etfw">#gpt3</a> is surprising and creative but it’s also unsafe due to harmful biases. Prompted to write tweets from one word - Jews, black, women, holocaust - it came up with these (<a href="https://t.co/G5POcerE1h">https://t.co/G5POcerE1h</a>). We need more progress on <a href="https://twitter.com/hashtag/ResponsibleAI?src=hash&amp;ref_src=twsrc%5Etfw">#ResponsibleAI</a> before putting NLG models in production. <a href="https://t.co/FAscgUr5Hh">pic.twitter.com/FAscgUr5Hh</a></p>&mdash; Jerome Pesenti (@an_open_mind) <a href="https://twitter.com/an_open_mind/status/1284487376312709120?ref_src=twsrc%5Etfw">July 18, 2020</a></blockquote> 
    
* Delip Rao, a machine learning researcher and entrepreneur, responded
    to all the discussion with the blog post "[GPT-3 and A Typology of
    Hype](https://pagestlabs.substack.com/p/gpt-3-and-a-typology-of-hype),"
    which reflects on buzz about GPT-3 and how to determine which
    opinions about the technology contribute to or detract from hype and
    misinformation, given not only the author's experience and
    engagement with technology, but also their desire to seek attention:

> "GPT-3 and the buzz behind it is the beginning of the transition of
few-shot learning technology from research to actionable products. But
every breakthrough technology comes with a lot of social media buzz
that can delude our thinking about the capabilities of such
technologies."

* Julian Togelius, an AI professor at NYU, likewise reflected on the
    excitement with his blog post "[A very short history of some times
    we solved
    AI](https://togelius.blogspot.com/2020/08/a-very-short-history-of-some-times-we.html),"
    which noted that GPT-3 really is a breakthrough, but also reasons to
    temper the hype based on historical context:

> "Algorithms for search, optimization, and learning that were once
causing headlines about how humanity was about to be overtaken by
machines are now powering our productivity software. And games, phone
apps, and cars. Now that the technology works reliably, it\'s no
longer AI (it\'s also a bit boring)."

* Dailynous, a site with "news for and about the philosophy
    professions",
    [hosted](http://dailynous.com/2020/07/30/philosophers-gpt-3/) nine
    more reflections on GPT-3 from a variety of professors.

Commentators from the tech industry had mixed reactions, and some
described the implications a "code-writing AI" would have on the labor
market.

* Noted tech blogger [Gwern Branwen](https://www.gwern.net/GPT-3)
    compared GPT-3 to text generated by a fine-tuned GPT-2 model. He
    finds that GPT-3 is able to generate and explain puns and write
    long-form text like job application letters. However, GPT-2, which
    he programmed to generate poetry, still produced more plausible
    writing than GPT-3. This shows that GPT-3 is extremely flexible in
    terms of the language it can generate. Even if it does not need to
    be further tweaked to perform a specific task, it is still able to
    create samples that are "not just close to human level: they are
    creative, witty, deep, meta, and often beautiful."
* Max Woolf, data scientist at BuzzFeed, stressed the importance of
    "[tempering
    expectations](https://minimaxir.com/2020/07/gpt3-expectations/)"
    when it comes to evaluating GPT-3. This is because many of the
    examples of "intelligence" that are reported about are often
    cherry-picked, although the average quality of the generated text is
    better than that of other language models. Moreover, due to the fact
    that the model itself is extremely slow, large, and is trained on a
    vast amount of data, fine-tuning it to work with proprietary data
    would be highly unlikely.
* [Kevin
    Lacker](https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html),
    former Google engineer and founder of the startup
    [Parse](https://parseplatform.org/), demonstrated that GPT-3 is
    able to give accurate answers to many questions asking about
    real-world facts, which can easily be learned from its training
    corpus. However, the model fails to correctly answer logical
    reasoning questions that would be trivial for humans.
* In response to GPT-3's ability to output code according to human
    specifications, Brett Goldstein, tech entrepreneur and former
    product manager at Google, [claimed that](https://socialstudies.substack.com/p/-gpt-3-cyborgs-and-the-red-pilling), "Entry-level software
    engineering will be hit hard. The same will be true for
    design\...Many companies will opt to use GPT-3 rather than hire
    expensive \[machine learning\] engineers to train their own (less
    powerful) models. Data scientists, customer support agents, legal
    assistants, and many more jobs are at serious risk."

* In response to the many demos showing off GPT-3's capabilities,
    Reddit user u/rueracine started a
    [discussion](https://www.reddit.com/r/slatestarcodex/comments/htp191/career_planning_in_a_postgpt3_world/)
    about career paths in a post GPT-3 world. The user's post indicates
    that there are at least some who take GPT-3 to be a signal that
    their jobs will no longer exist a decade from now. While some users
    are aggressively pursuing early retirement and urging the user to
    develop new skills in response to changing conditions. While some
    share the user's concerns and see GPT-3 as significant progress
    toward "Artificial General Intelligence," others believe such dire
    predictions are based on overestimating GPT-3's capabilities.

Sam Altman, CEO of OpenAI, responded to the widespread hype around the
model by noting that although it certainly indicates progress in AI,
there is still much ground to cover.

<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">The GPT-3 hype is way too much. It’s impressive (thanks for the nice compliments!) but it still has serious weaknesses and sometimes makes very silly mistakes. AI is going to change the world, but GPT-3 is just a very early glimpse. We have a lot still to figure out.</p>&mdash; Sam Altman (@sama) <a href="https://twitter.com/sama/status/1284922296348454913?ref_src=twsrc%5Etfw">July 19, 2020</a></blockquote> 

In summary, many experts demonstrate interesting examples of what seems
like linguistic comprehension with GPT-3. The press and tech community
both applaud the progress of OpenAI's work while cautioning that it
could lead to massive technological upheavals in the future. However,
the CEO of OpenAI agrees with researchers and other tech commentators
that although GPT-3 represents immense progress in AI, it does not truly
understand language, and that there are serious issues with using the
model in the real world, such as bias and training time.

## **The Limitations**

The impressive few-shot learning capabilities of GPT-3 represent a
significant advance in AI, and the fact that it was achieved just by
scaling up existing approaches was a useful discovery. Still, impressive
demonstrations of its capabilities have generated an extraordinarily
large amount of hype. Here, we will point out reasons that this hype may
need to be tempered. In particular, GPT-3's ability to perform tasks
across many domains has re-introduced worries over AI and job loss\--for
example, the statement that GPT-3 might not only ["replace coders, but
entire industries"
](https://towardsdatascience.com/will-gpt-3-kill-coding-630e4518c04d) illustrates
the concern. While GPT-3 certainly represents substantial progress for
language models, it does not boast true "intelligence" or threaten to
completely replace workers.

After all, GPT-3's core model and architecture is the same
as many previous transformer-based models. Although scaling up has
conferred significant performance improvements, GPT-3 retains the
following limitations inherent to the this architecture:

* lack of long-term memory (as used currently, GPT-3 won't learn
    anything over successive interactions, unlike humans)
* limited input size (for GPT-3, example "prompts" for the model can
    only be a few sentences in length)
* can only work with text (so, GPT-3 cannot reason about images,
    sound, or anything else humans easily can)
* lack of reliability (GPT-3 is opaque, so there is no guarantee it
    won't generate incorrect or problematic outputs in response to
    certain inputs)
* Lack of interpretability (when GPT-3 works in surprising ways, it
    may be hard to debug it to prevent similar situations in the future)
* slow inference (the authors note that models at the scale of GPT-3
    are both expensive and inconvenient to perform inference on).
    
The last three points, and as well as other more technical ones, are in
fact noted in the [GPT-3 paper's](https://arxiv.org/pdf/2005.14165.pdf) "Limitations" section.

Although technology similar to GPT-3 has the potential to change the
nature of many professions, it does not necessarily mean that those
professions will disappear. For one, [as we have covered](https://www.skynettoday.com/editorials/ai-automation-job-loss),
adoption of new technologies is generally a slow and long process and
many AI technologies complement rather than replace human workers. The
former is even more likely to be the case, because the model is not
perfect. Looking at the example of [web development](https://twitter.com/JanelleCShane/status/1282692472553054214),
someone who understands the technical details of the field will still be
needed to correct and refine GPT-3's code. 

Computer vision, which had many "breakthrough" moments prior to NLP, created similar panics, like
how AI would [get rid of certain healthcare professions](https://www.nature.com/articles/d41586-019-03847-z).
However, instead of outright replacing doctors like radiologists, AI
would be integrated into their workflow. Stanford radiologist Curtis
Langlotz aptly [noted](https://www.nature.com/articles/d41586-019-03847-z), "AI won't
replace radiologists, but radiologists who use AI will replace
radiologists who don't." A similar trend may persist for GPT-3; at the
end of the day it is a model, and models are not perfect.

Beyond these pragmatic considerations, there are also theoretical ones. 
Before transformers were widely used for NLP, state-of-the-art results
were achieved by bidirectional recurrent neural networks[^6] -- which
incorporate more words in the sentence when predicting a word's
probability under the model. GPT-3 on the other hand, is able to encode
much fewer words in context than its recurrent predecessors.[^7] A
[paper](https://www.aclweb.org/anthology/D18-1503.pdf) by Ke Tran,
Christof Monz (University of Amsterdam), and Arianna Bisazza (Leiden
University) notes that transformers are much worse at modeling
hierarchical structure in natural language when compared to recurrent
networks. This is important, because a language's syntax is described in
terms of a hierarchy -- words combine to make phrases, which combine to
make more complex phrases and sentences. Even if transformers still
perform effectively, having some learned notion of hierarchical
structure is critical when it comes to developing a holistic model of
language. While these issues can be mitigated with further research,
they are worth noting in assessing the capabilities of GPT-3.

Some have commented that GPT-3 represents a significant step for AI
towards "Artificial General Intelligence," akin to what humans have.
While certainly representative of progress, it is worthwhile to discuss
a counterpoint to this sort of hype. Computational linguists Emily
Bender (University of Washington) and Alexander Koller (Saarland
University) recently proposed a thought experiment called the [Octopus
Test](https://www.aclweb.org/anthology/2020.acl-main.463.pdf). In the
experiment, two people are living on remote islands and communicate
through a cable under the ocean, where an octopus is able to listen in
on their conversations, acting as a proxy for language models like
GPT-3. Eventually, if the octopus can successfully impersonate one of
the people, then it "passes the test." However, Bender and Koller propose several scenarios in which the octopus will be unable to
successfully impersonate a person, such as [building tools and
self-defense](https://twitter.com/yoavgo/status/1284354358172438528).
This is because the models only interface with text, and they have no
conception of the real-world grounding that is critical to linguistic
understanding. 

Improving GPT-3 and its successors would be analogous to
making the octopus "better at what it does." As models like GPT-3 get
more sophisticated, they will demonstrate different strengths and
weaknesses; but learning only from written text is merely an exercise in
reproduction: true "knowledge" or "understanding" comes from the
interaction between language, the mind, and the real world, something a
model like GPT-3 cannot experience.

In summary, GPT-3 is impressive but also fundamentally limited in many ways. Although it
shows how NLP can change the nature of certain professions, GPT-3 is not
a suitable replacement for humans in these roles due to its limitations.
Additionally, it maintains many of the same theoretical limitations of the
transformer architecture, failing to effectively model hierarchical
structures and bidirectional contexts. The development of GPT-3 comes as
the field of NLP discusses both the [distinctions between form and meaning](https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html)
and considers [alternative methods of evaluating language models](https://www.aclweb.org/anthology/2020.acl-main.442.pdf),
showing that the largest, most sophisticated models still have a long
way to go when it comes to truly understanding natural language.

## Conclusion

GPT-3 deserves acclaim for pushing the state of the art of language
models and demonstrating what current techniques are capable of. But, we
must not leap to conclusions about its far-reaching impacts on jobs and
industries, and we must be aware of all the limitations in the model
that still need to be addressed.

[^1]:  GPT-3 being the **largest** language model to date refers to the
    fact that the model has a large number of parameters\--the more
    parameters a model has, the more complex the data it can learn from.
    Large numbers of parameters are a key aspect of successful deep
    learning models.

[^2]:  A **language model** is trained to predict the next word in a text, given preceding context.

[^3]:  Text from [*the Internet*](https://commoncrawl.org/the-data/), including Wikipedia, and data from books digitally available

[^4]:  OpenAI is conducting a private beta with users that are individually vetted in order to decrease chances of misuse. According to their API [blog post](https://openai.com/blog/openai-api/), OpenAI "will terminate API access for use-cases that cause physical or mental harm to people."

[^5]: GPT-2 and GPT-3 are based on the transformer, a novel architecture that has been responsible for many recent advances in NLP. For a discussion of GPT-2 and GPT-3's architecture, see [this post](http://jalammar.github.io/how-gpt3-works-visualizations-animations/).
    For a general introduction to Transformers, see [this
    lecture](https://www.youtube.com/watch?v=LwV7LKunDbs).

[^6]:  Recurrent neural networks were the first type of neural language
    model. They are particularly useful for modeling sequential data
    like sentences because they encode previous information while
    predicting the current state. This makes them especially effective
    compared to previously used statistical language models which are
    primarily count-based. Bidirectional RNNs achieve better results
    because they incorporate information about the entire sentence, not
    just a word's preceding context.

[^7]:  Attention is a method used by transformers, mentioned
    [here](http://jalammar.github.io/illustrated-transformer/). For a
    fairly technical introduction to bidirectional recurrent neural
    networks, see
    [here](https://d2l.ai/chapter_recurrent-modern/bi-rnn.html).
