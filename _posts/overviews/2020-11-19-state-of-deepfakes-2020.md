---
title: "The State of Deepfakes in 2020"
author: [eric_hofesmann]
categories: [overviews]
tags: [DeepFakes]
excerpt: "On what DeepFakes are, how they are used, and how much they should concern you."
image: 
  feature: assets/img/overviews/2020-11-20-state-of-deepfakes-2020/main.webp
  credit: Facebook
permalink: /overviews/state-of-deepfakes-2020
highlight: true
sidebartoc: true
---

> “The rise of synthetic media and deepfakes is forcing us towards an important and unsettling
realization: our historical belief that video and audio are reliable records of reality is no
longer tenable.” -[The State of DeepFakes 2019 Report](https://sensity.ai/mapping-the-deepfake-landscape/)

Media manipulation through images and videos has been around for decades. For example, during [WWII, Mousollini released a propaganda image](https://www.dailymail.co.uk/news/article-4984364/How-Hitler-Mussolini-Lenin-used-photo-editing.html) of himself on a horse with his horse handler edited out. The goal was to make himself seem more impressive and powerful [^1]. These types of tricks can have significant impacts given the scale of the audience, especially in the internet era. [DARPA has constructed an entire program](https://www.darpa.mil/program/media-forensics) to develop media forensics methods for detecting manipulated media [^2].

<figure>
    <img class="postimagehalf" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image10.webp" alt="Mussolini1"/> 
    <img class="postimagehalf" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image14.webp" alt="Mussolini2"/> 
    <figcaption>Image of Mousollini with his horse handler edited out to make himself seem more impressive <a href="https://www.dailymail.co.uk/news/article-4984364/How-Hitler-Mussolini-Lenin-used-photo-editing.html">(source)</a></figcaption>    
</figure>

[Fake news may one day pale in comparison to the impact of deepfake news](https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth) [^3]. Deepfakes are a set of Computer Vision methods that can create doctored images or videos with uncanny realism. In recent years, they have been blowing up in both quality and popularity. The term deepfake comes from a “fake” image or video generated by a “deep” learning algorithm. You’ve likely seen a video of a movie scene with actors face-swapped with a scary degree of accuracy.

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image2.gif" alt="Mussolini1"/> 
    <figcaption>Deepfake of Arnold Schwarzenegger and Silvester Stallone in the movie Step Brothers <a href="https://www.youtube.com/watch?v=uXwmSFjlVc0">(source)</a></figcaption>    
</figure>

This technology has the potential to provide malicious actors with the means to sow unprecedented amounts of disinformation because it is far cheaper and more accessible than traditional special effects techniques used in Hollywood when it comes to creating videos with realistic face-swaps. This is concerning, since fake news is already prevalent and people are all too often believing news with little to no evidence [^4]. Just recently, [a deceptively-edited video making it look like Joe Biden didn’t know what state he is in got 1 million views on Twitter](https://www.cnn.com/2020/11/01/tech/false-biden-video-twitter/index.html) [^5]. Deepfakes could provide “evidence” to people who are looking to further their cognitive dissonance around disinformation. This is a real threat, with two bills [H. R. 3230](https://www.congress.gov/bill/116th-congress/house-bill/3230/text),[S. 3805](https://www.congress.gov/bill/115th-congress/senate-bill/3805/text) having already been proposed in Congress to counter the spread of deepfakes for illegal purposes.

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image4.webp"/> 
    <figcaption>The number of detected deepfakes as of July 2020 <a href="https://sensity.ai/deepfake-threat-intelligence-a-statistics-snapshot-from-june-2020/">(source)</a></figcaption>
</figure>

Since the term was coined in 2017, the amount of detected deepfakes on the internet has been increasing exponentially. At the same time, the underlying AI methods to generate deepfakes have improved, as have user-friendly tools that allow one to easily create deepfakes without in-depth technical knowhow. In this blog post, I’m going to go over some background behind deepfakes, what they have been used for, and how to counter them.

## What are deepfakes?

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image3.webp"/> 
    <figcaption>Progress of GANs over the last few years  <a href="https://twitter.com/goodfellow_ian/status/1084973596236144640?s=20">(source)</a></figcaption>
</figure>

> A deepfake refers to a specific kind of synthetic media where a person in an image or video is swapped with another person's likeness. -[Meredith Somers](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained)

Synthetic image and video generation have been a growing computer vision subfields, and they got a lot of momentum with the introduction of generative adversarial networks (GAN) in 2014 [^8]. The term “deepfake” was first coined by a Reddit user in 2017 who was using the technology to create fake pornography of celebrities using face swaps [^10]. Since then, the term has evolved to cover a wider range of image and video augmentations and generation as well as some audio applications.

Deepfakes come in a variety of forms according to [Mirsky and Lee](https://arxiv.org/pdf/2004.11138.pdf#page=34&zoom=100,65,305) [^9]. Here are examples of different types of deep fakes on a woman’s face:

### Reenactment 
aka Using your facial or body movements to dictate the movements of another person

Current works in reenactment are looking to minimize the amount of training data that is needed to be able to generate a modified face. One-shot and few-shot learning have become popular approaches to utilize data from many different faces but only need to fine tune on a select few samples of their target face. Some of the leading methods here are [MarioNETte](https://arxiv.org/pdf/1911.08139.pdf) and [FLNet](https://arxiv.org/pdf/1911.09224.pdf).

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image11.webp"/> 
    <figcaption><a href="https://arxiv.org/pdf/2004.11138.pdf">(source)</a></figcaption>    
</figure>

### Replacement 
aka Your identity is mapped to another person (for example the face swap filter on Snapchat)

Many replacement works apply either encoder-decoder networks or variational autoencoders to learn to map the source face to the target while retaining the same expressions. Common challenges that come up with replacement is the need to deal with occlusions, when an object passes in front of the target’s face. Recent replacement models include [FaceShifter](https://arxiv.org/abs/1912.13457) and [FaceSwap-GAN](https://github.com/shaoanlu/faceswap-GAN).

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image5.webp"/> 
    <figcaption><a href="https://arxiv.org/pdf/2004.11138.pdf">(source)</a></figcaption>
</figure>

### Editing
aka Altering the attributes of a person in some way (like changing their age or glasses)

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image8.webp"/> 
    <figcaption><a href="https://arxiv.org/pdf/2004.11138.pdf">(source)</a></figcaption>
</figure>

### Synthesis
aka Generating completely new images or videos with no target person

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image9.webp"/> 
    <figcaption><a href="https://arxiv.org/pdf/2004.11138.pdf">(source)</a></figcaption>
</figure>

Reenactment and replacement are the two most prominent types of deepfakes, since they have the highest potential for destructive applications. For example, the face of a politician could be reenacted to say something they never actually said, or a person’s face could be replaced into an incriminating video for blackmail purposes. These types of manipulated media (deepfake or not) can be incriminating and/or harmful to the target’s reputation, like the [post of Biden mentioned earlier](https://www.cnn.com/2020/11/01/tech/false-biden-video-twitter/index.html) in which signs that said the name of the state were edited to make it look like he didn’t know where he was.

At the core of the concern about deepfakes is the fact that they make reenactment and replacement accessible and cheap; you don’t have to understand the details of the techniques involved to be able to create deepfakes. Tools like [DeepFaceLab](https://github.com/iperov/DeepFaceLab) allow anyone to take an image or video and replace a face, de-age a face, or manipulate a speech. For really seamless deepfakes, tools like this will provide a high quality generation of a face, but you still need some experience in video editing software like Adobe After Effects to add them to a video. If you don’t have the desire or ability to make them yourself, you can even find services and marketplaces willing to create deepfakes for you. One example is [https://deepfakesweb.com/](https://deepfakesweb.com/) where you just have to upload videos and images you want, and they will create a deepfake for you in the cloud.

## How are deepfakes being used?

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image1.webp"/> 
    <figcaption>Data as of 2019 from a total off 14,678 detected videos <a href="https://sensity.ai/mapping-the-deepfake-landscape/">(source)</a></figcaption>
</figure>

When deepfakes first gained popularity in 2017, they were overwhelmingly used in internet forums to generate fake pornography of celebrities. Even in 2019, 96% of all publicly posted deepfakes were pornographic, according to a [survey](https://sensity.ai/mapping-the-deepfake-landscape/) done by the company [sensity](https://sensity.ai/about/). The targets of deepfakes (the people who have their faces swapped into deepfakes) are generally people who work in the entertainment industry.

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image13.webp"/> 
    <figcaption>Distribution of deepfake targets as of June 2020 from a total of 49,081 detected videos <a href="https://sensity.ai/deepfake-threat-intelligence-a-statistics-snapshot-from-june-2020/">(source)</a></figcaption>
</figure>

### Deepfakes in Media

Aside from pornography, various online creators have been applying deepfakes to other forms of media, namely movie clips; individuals are constantly producing viral videos face swapping actors of movies with other actors. For example, in a recent YouTube video the faces of Arnold Schwarzenegger and Silvester Stalone were added to the movie Step Brothers [^11].

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image2.gif"/> 
    <figcaption>Deepfake of Arnold Schwarzenegger and Silvester Stallone in the movie Step Brothers <a href="https://www.youtube.com/watch?v=uXwmSFjlVc0">(source)</a></figcaption>
</figure>

While such videos suggest deepfakes could prove to be useful for the entertainment industry, most of these viral videos have just been short demonstrations, and there has yet to be a major Hollywood production to utilize deepfakes. Though, some might say that Hollywood should already be using deepfakes. Netflix received a lot of criticism for the quality of the de-aging effect that was used in the movie “The Irishman”. In response, someone [applied a de-aging deepfake to the movie and got impressive results](https://www.creativebloq.com/news/the-irishman-deepfake); perhaps it’s only a matter of time until Hollywood productions use these tools directly.

While no large blockbusters have used deepfakes, there are some accounts of smaller productions experimenting with deepfakes. Most notably, a producer has used deepfakes in his [documentary about gay and lesbian persecution in Chechnya, to disguse the identities of the people being interviewed](https://www.nytimes.com/2020/07/01/movies/deepfakes-documentary-welcome-to-chechnya.html).

As the examples above demonstrate, besides the possible negative uses of deepfakes, they can also lead to new possibilities when it comes to the entertainment industry. For instance, it could be really cool to be able to select which actor you want to watch in a movie. And, comedy impersonations like on Saturday Night Live will no doubt also reach a whole new level with deepfakes. In fact, just the other week, the creators of South park, Trey Parker and Matt Stone, started a new comedy series on YouTube called “Sassy Justice” centered around using deepfakes for celebrity impersonations. If you’re a fan of South Park you should definitely check it out:

<figure> 
    <div class="youtube" data-embed="dCTM2lvm0QE">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>

    </div>
</figure> 

### Deepfakes in Politics

Whenever a deepfake video gains attention online, there are always numerous comments from people expressing their concerns about using deepfakes to influence politics for nefarious purposes. These concerns are definitely valid, since in the information age being able to flood the internet with fake media about your political opponents that can fool viewers eyes and ears could be disastrous. To date, there have been quite a few deepfakes made addressing politics, many of which are satirical. One of the most popular was made by Jordan Peele where he reenacted Obama’s face [^12]. This was not done maliciously, but rather to raise awareness of the potential that deepfakes have to shape the political landscape.

<figure> 
    <div class="youtube" data-embed="cQ54GDm1eL0">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>
    <figcaption>Deepfake example</figcaption>    
    </div>
</figure> 


Another example of using deepfakes to make a political statement are these videos of Kim Jong-Un and Vladimir Putin [discussing the election and the need for a peaceful transition](https://www.technologyreview.com/2020/09/29/1009098/ai-deepfake-putin-kim-jong-un-us-election/) of power.

<figure> 
    <div class="youtube" data-embed="ERQlaJ_czHU">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>
    </div>
    <figcaption>Deepfake example</figcaption>    
</figure> 

<figure> 
    <div class="youtube" data-embed="sbFHhpYU15w">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>
    </div>
    <figcaption>Deepfake example</figcaption>    
</figure> 

Only a few cases have been uncovered so far of non-satirical deepfakes being used to influence politics. But, just because a deepfake is made with the intent to influence politics does not automatically mean it is malicious. Positive deepfakes can be used, for example, to let politicians reach a wider audience. An Indian politician, Manoj Tiwari, made a deepfake of an announcement he made to translate it into other languages, like English [^13].


<figure> 
    <div class="youtube" data-embed="88GUbuL89bQ">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>
    </div>
    <figcaption>Deepfake</figcaption>    
</figure> 

<figure> 
    <div class="youtube" data-embed="2Tar2O4q0qY">
      <!-- (2) the "play" button -->
      <div class="play-button"></div>
    </div>
    <figcaption>Original</figcaption>    
</figure> 

In addition to this deepfake of Tiwari, a decent portion of detected deepfakes are targeting India. Though, the overwhelming majority of detected deepfakes are targeting the US and UK. With the creation of deepfakes spreading all across the globe, deepfakes have the potential to influence far more than just US politics.

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image6.webp"/> 
    <figcaption>Distribution of detected deepfakes by targeted country <a href="https://sensity.ai/deepfake-threat-intelligence-a-statistics-snapshot-from-june-2020/">(source)</a></figcaption>
</figure>

While no wars have been fought due to deepfakes, there have been instances of deepfakes being used to attack political opponents and ideas. A [deepfake of Donald Trump speaking](https://journalism.design/deepfakes/extinction-rebellion-sempare-des-deepfakes/) about the Paris climate agreement was produced by a Belgian political group to persuade people to sign a petition calling the government to take more action against climate change [^3, 14].

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image12.gif"/> 
    <figcaption>Trump deepfake <a href="https://www.facebook.com/Vlaamse.socialisten/videos/10155618434657151/">(source)</a></figcaption>
</figure>

Even though the quality of the deepfake is rather poor and the deepfake can easily be spotted by looking at his mouth, numerous commenters were deceived and were calling out Trump for the statements in the video. As deepfakes are increasing in quality, it not only becomes concerning that you may not be able to tell if a video was faked, but also that someone can claim a real incriminating video is actually fake. Last year, [a Malaysian politician was jailed over a video of him engaging in homosexual activity](https://www.sbs.com.au/news/a-gay-sex-tape-is-threatening-to-end-the-political-careers-of-two-men-in-malaysia) (which is illegal in Malaysia) that the politician claims is a deepfake. Experts were not able to concretely determine if the video was faked or not. In the future, this defense of claiming a real video as a deepfake could be used for much more serious crimes.

## Should I be worried?

As of today, there have not been any notable deepfakes that have targeted the 2020 US Election [^3][^4], and in general fake news and disinformation are still bigger concerns [^5]. That is, videos created by manual editing, like one showing Biden asleep during a TV interview [^16], are a much greater concern than what is output by deep learning models currently. On top of that, the largest sources of disinformation are through rumors and forums, rather than any individual video [^17].

Still, many are predicting deepfakes will inevitably make disinformation much worse that it is now. Luckily, as deepfake technology has progressed, so have methods for detecting them. There have been multiple competitions hosted on Kaggle designed to create better deepfake detectors [^6]. Additionally, a number of datasets have been collected to help train these detectors [^18][^19][^20].

Deepfake detectors work by being trained to find a host of different deepfake identifiers in images and videos. For example, [Face X-ray](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Face_X-Ray_for_More_General_Face_Forgery_Detection_CVPR_2020_paper.pdf) is a method designed to look for boundaries when a deepfake face is blended back into the target image or video. [Another method](https://arxiv.org/abs/2008.12262) looks at the background of the image and compares it with the face to see if there are discrepancies between the two. [An emotion recognition network](https://arxiv.org/abs/2003.06711) has also been used to detect if the emotions shown on the face match the context and audio of the scene to determine if the behavior of the actor indicates a deepfake. Another way that human biology can be used to detect deepfakes and even identify the [deepfake model used is by detecting heartbeats from the target of the video](https://venturebeat.com/2020/09/03/ai-researchers-use-heartbeat-detection-to-identify-deepfake-videos/) and analyzing the residuals of that signal. Most of these deepfake detection works have open-source code available, however, there are also some commercially available APIs, for example [sensity](https://sensity.ai/api-2/), [deepware](https://deepware.ai/developer/), and [Microsoft Video Authenticator](https://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authenticator/).

In the literature, these detectors are compared based on their accuracy at detecting real or fake videos on common deepfake datasets. Some of the best models are able to achieve upwards of [99% accuracy on these datasets](https://www.researchgate.net/profile/Md_Rana25/publication/343751402_DeepfakeStack_A_Deep_Ensemble-based_Learning_Technique_for_Deepfake_Detection/links/5f435949458515b729498d4b/DeepfakeStack-A-Deep-Ensemble-based-Learning-Technique-for-Deepfake-Detection.pdf). This indicates that these datasets have been solved and there needs to be more research done to generate new datasets using state-of-the-art deepfake technologies. Since these methods rarely show results on deepfakes “in the wild,” I decided to take the winner of a Kaggle competition and try it out on some of the deepfakes shown in this post.

<figure>
    <img class="postimagesmall" src="/assets/img/overviews/2020-11-20-state-of-deepfakes-2020/image7.webp"/> 
</figure>

Above are [deepfake detector](https://www.kaggle.com/vaillant/dfdc-3d-2d-inc-cutmix-with-3d-model-fix) results on videos in this post shown in [our](https://voxel51.com/) ML visualization tool [FiftyOne](https://voxel51.com/docs/fiftyone/). Download this [example here](https://github.com/voxel51/fiftyone-examples/blob/master/examples/deepfakes_in_politics.ipynb).

[This deepfake detection model](https://www.kaggle.com/vaillant/dfdc-3d-2d-inc-cutmix-with-3d-model-fix) was not able to identify the Step Brothers or Donald Trump videos as fake. The Step Brothers example is pretty seamless even to a human observer, but the video of Donald Trump is clearly a fake if you pay attention to the artifacts around the mouth. Thus, even though these models are performing well on common deepfake detection datasets, using them in real world scenarios is not as consistent as one would hope. If you are interested in performing such exploration yourself, you can check out the [Jupyter notebook write-up](https://github.com/voxel51/fiftyone-examples/blob/master/examples/deepfakes_in_politics.ipynb) showing the process and results.

This suggests that there is a need to continue research in deepfake detectors before deepfakes become more prevalent. Luckily, high-quality deepfakes still require human intervention to touch up the output, which will slow their spread and give deepfake detection models time to improve.

### Spot the deepfake 

The best way to be able to prevent deepfakes from being used maliciously is to train people to [spot the tell tale signs of a deepfake](https://www.spotdeepfakes.org/en-US) so they don’t accidentally fall for it. While some deepfakes are really well made, there are oftentimes common traits that can give them away.

According to MIT researcher [Matt Groh](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained), if you think that you might be seeing a deepfake, you should look particularly closely at the:

- Face: Are parts of the face seeming to “glitch”? Are their eyebrows and facial muscles moving when they talk? Does their hair seem to be falling naturally?
- Audio: Does the audio match up with the expressions on the person’s face? Are there any weird cuts or jumps that sound unnatural in the audio?
- Lighting: Is there a portion of the video where the lighting doesn’t match the rest of the scene? Are you seeing reflections in things like glasses?

You can see how good you are at spotting deepfakes by checking out [detectfakes.media.mit.edu](http://detectfakes.media.mit.edu). This website provides examples from the Deepfake Detection Kaggle challenge to test if you can spot the deepfake as good as a deepfake detection model.

## Summary

Deepfakes can lead to a potentially terrifying future of misinformation in the media. However, the current state of the technology is still in its infancy and high-quality deepfakes require a significant human touch. As deepfakes are improving, so are the methods and algorithms to counter them. At least for now, a much greater concern is the spread of misinformation via manually edited images and videos.


<br>
<br>
<hr>

**About Me** 

My name is Eric Hofesmann. I received my master’s in Computer Science, specializing in Computer Vision, at the University of Michigan. During my graduate studies, I realized that it was incredibly difficult to thoroughly analyze a new model or dataset without serious scripting to visualize and search through outputs and labels. Working at the computer vision startup,[Voxel51](http://voxel51.com), I helped develop the tool [FiftyOne](https://voxel51.com/docs/fiftyone/) so that researchers can quickly load up and start looking through datasets and model results. Follow me on twitter [@ehofesmann](https://twitter.com/ehofesmann)

**Citation**

This piece is an updated and expanded version of blog posts originally released in 2015 on www.andreykurenkov.com. Please cite this version.

For attribution in academic contexts or books, please cite this work as

> Eric Hofesmann, "The State of Deepfakes in 2020", Skynet Today, 2020.

BibTeX citation:

<blockquote>
<p>@article{hofesmann2020state,<br/>
  author = {Hofesmann, Eric},<br/>
  title = {The State of Deepfakes in 2020},<br/>
  journal = {Skynet Today},<br/>
  year = {2020},<br/>
  howpublished = {\url{<a href="https://skynettoday.com/overviews/state-of-deepfakes-2020">https://skynettoday.com/overviews/state-of-deepfakes-2020</a> } },<br/>
}</p>
</blockquote>

[^1]: S. Malm,[How Hitler, Mussolini, Lenin and Chairman Mao used photo editing to aid their propaganda: Before-and-after images reveal how they carefully managed their image](https://www.dailymail.co.uk/news/article-4984364/How-Hitler-Mussolini-Lenin-used-photo-editing.html),Daily Mail (2017)

[^2]: [Media Forensics](https://www.darpa.mil/program/media-forensics) (MEDIFOR), DARPA

[^3]: O. Schwartz [You thought fake news was bad? Deep fakes are where truth goes to die](https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth), The Guardian (2018)

[^4]: G. Shao, [Fake videos could be the next big problem in the 2020 elections](https://www.cnbc.com/2019/10/15/deepfakes-could-be-problem-for-the-2020-election.html), CNBC (2019)

[^5]: D. O’Sullivan, [False video of Joe Biden viewed 1 million times on Twitter](https://www.cnn.com/2020/11/01/tech/false-biden-video-twitter/index.html), CNN (2020)

[^6]: [Deepfake Detection Challenge](https://www.kaggle.com/c/deepfake-detection-challenge/leaderboard), Kaggle (2020)

[^7]: [Voxel51](http://voxel51.com),[ *FiftyOne: Explore, Analyze and Curate Visual Datasets](https://github.com/voxel51/fiftyone), (2020)

[^8]: I. Goodfellow, et al.,[Generative adversarial nets](http://papers.nips.cc/paper/5423-generative-adversarial-nets), NeurIPS (2014)

[^9]: Y. Mirsky, W. Lee,[ *The Creation and Detection of Deepfakes: A Survey](https://arxiv.org/abs/2004.11138), arXiv (2020)

[^10]: S. Cole, [We Are Truly Fucked: Everyone Is Making AI-Generated Fake Porn Now](https://www.vice.com/en/article/bjye8a/reddit-fake-porn-app-daisy-ridley), Vice (2018)

[^11]: [B. Monarch, Instagram](https://www.instagram.com/brianmonarch/) (2020)

[^12]: J. Vincent, [Watch Jordan Peele use AI to make Barack Obama deliver a PSA about fake news](https://www.theverge.com/tldr/2018/4/17/17247334/ai-fake-news-video-barack-obama-jordan-peele-buzzfeed), The Verge (2018)

[^13]: N. Christopher, *We’ve Just Seen the First Use of Deepfakes in an Indian Election Campaign](https://www.vice.com/en/article/jgedjb/the-first-use-of-deepfakes-in-indian-election-by-bjp), Vice (2020)

[^14]: G. Holubowicz, [Extinction Rebellion takes over deepfakes](https://journalism.design/deepfakes/extinction-rebellion-sempare-des-deepfakes/), Journalism.Design (2020)

[^15]: B. Dolhansky, et al., [The DeepFake Detection Challenge (DFDC) Dataset](https://arxiv.org/abs/2006.07397), arXiv (2020)

[^16]: [White House social media director tweets manipulated video to depict Biden asleep in TV interview](https://www.washingtonpost.com/video/politics/white-house-social-media-director-tweets-manipulated-video-to-depict-biden-asleep-in-tv-interview/2020/09/02/4c71391a-44bd-40e0-91ab-e98077e9b17b_video.html), Washington Post (2020)

[^17]: M. Parks, [Fake News: How to spot misinformation](https://www.npr.org/2019/10/29/774541010/fake-news-is-scary-heres-how-to-spot-misinformation), NPR (2019)

[^18]: Y. Li, et al., [Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics](https://arxiv.org/pdf/1909.12962.pdf) CVPR (2020)

[^19]: A. Rossler, et al., Faceforensics++: [Learning to detect manipulated facial images](https://arxiv.org/abs/1901.08971). ICCV (2019)

[^20]: H. Dang, et al., [On the detection of digital face manipulation](https://arxiv.org/pdf/1910.01717.pdf). CVPR (2020)

**Tools Used** 

- [FiftyOne for data visualization and exploration](https://github.com/voxel51/fiftyone)

- [Kaggle challenge finalist](https://www.kaggle.com/vaillant/dfdc-3d-2d-inc-cutmix-with-3d-model-fix)

**Data Sources**


-   [Kaggle data](https://www.kaggle.com/c/deepfake-detection-challenge/data)

-   [https://www.youtube.com/watch?v=ai01D82uBs8](https://www.youtube.com/watch?v=ai01D82uBs8)

-   [https://www.youtube.com/watch?v=cQ54GDm1eL0](https://www.youtube.com/watch?v=cQ54GDm1eL0)

-   [https://www.youtube.com/watch?v=2Tar2O4q0qY&feature=emb\_title](https://www.youtube.com/watch?v=2Tar2O4q0qY&feature=emb_title)

-   [https://www.youtube.com/watch?v=88GUbuL89bQ&feature=emb\_title](https://www.youtube.com/watch?v=88GUbuL89bQ&feature=emb_title)

-   [https://www.facebook.com/Vlaamse.socialisten/videos/10155618434657151/](https://www.facebook.com/Vlaamse.socialisten/videos/10155618434657151/)

-   [https://www.youtube.com/watch?v=UEO\_NAOqDa8](https://www.youtube.com/watch?v=UEO_NAOqDa8)

-   [www.youtube.com%2Fwatch%3Fv%3DuXwmSFjlVc0%26feature%3Demb\_title](https://www.youtube.com/watch?v=uXwmSFjlVc0&feature=emb_title)




