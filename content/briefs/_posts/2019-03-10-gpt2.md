---
layout: post
image:
  feature: content/briefs/images/gpt2/main.png
  credit: Delip Rao, <a href="http://deliprao.com/archives/314">When OpenAI tried to build more than a Language Model</a>
title: "OpenAI’s GPT2 - Food to Media hype or Wake Up Call?"
excerpt: "An effort to encourage the AI research community to talk about responsible disclosure of technology was met by strong criticism"
author: [andrey_kurenkov, arnav_arora]
tags: [OpenAI,hype]
permalink: /briefs/gpt2
---

## What Happened

<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">We&#39;ve trained an unsupervised language model that can generate coherent paragraphs and perform rudimentary reading comprehension, machine translation, question answering, and summarization — all without task-specific training: <a href="https://t.co/sY30aQM7hU">https://t.co/sY30aQM7hU</a> <a href="https://t.co/360bGgoea3">pic.twitter.com/360bGgoea3</a></p>&mdash; OpenAI (@OpenAI) <a href="https://twitter.com/OpenAI/status/1096092704709070851?ref_src=twsrc%5Etfw">February 14, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

On February 14 of 2019 the non-profit AI research company [OpenAI](https://en.wikipedia.org/wiki/OpenAI)
released the blog post [Better Language Models and Their Implications](https://blog.openai.com/better-language-models/), which
covered new research based on a scaled up version of their transformer-based
language model [^technical] initially released in June 2018. 
The model, called GPT-2 [^GPT], was shown to be capable of writing long form coherent passages after being provided a short prompt. This is an impressive feat that previous models have struggled to do effectively but something that the [field was moving towards](http://approximatelycorrect.com/2019/02/17/openai-trains-language-model-mass-hysteria-ensues/).

[^GPT]: Generative Pre-Training Transformer 2; an overview of the original GPT model can be [read here](https://blog.openai.com/language-unsupervised/).

<figure>
  <img class="image" src="/content/briefs/images/gpt2/unicorn.png" alt="Unicorn story."/>
  <figcaption>
    The first prompt and example output highlighted in <a href="https://blog.openai.com/better-language-models/">OpenAI's blog post</a>.
  </figcaption>
</figure>

In addition to presenting these impressive new results, the post commented on several 
potential misuses of their state-of-the-art model (such as generating misleading news articles, impersonating others online, and large-scale production of spam) and explained that OpenAI chose to pursue an unusually closed release strategy because of these potential misuses: 

> "Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code. We are not releasing the dataset, training code, or GPT-2 model weights... This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community."

This goes against what is increasingly the norm in AI research --- when sharing new findings, it is now typical to also share data, code, and pre-trained models for running the relevant experiments. This has been one of the main reasons the field has progressed so rapidly over the past decade, as researchers don't have to spend months reproducing each other's work
and can verify and build on new results quickly. Although this has been benefitial, many in the field have also started to discuss the potential of open source code and pre-trained models for dual-use [^dual_use] and the implications of that for how researchers should share new developments.

[^technical]: A transformer is a popular new idea for machine learning with language -- read more [here](http://jalammar.github.io/illustrated-transformer/). A language model is an AI algorithm meant to injest sequences of text and predict what words are most likely to occur next.
[^dual_use]: Technologies which are designed for civilian purposes but which may have military applications, or more broadly designed for certain beneficial uses but can be abused for negative impacts

Such a coherent set of output, as well as OpenAI’s strong stance on
not releasing their model, data, and code, were notable enough to turn
much attention towards it. This alone would have been enough to incite much discussion, 
but there was a second element to this event: prior to the AI research community being aware of
this new work, multiple journalists were informed of it and provided with access to write articles to be released soon after the blog post.
The flurry of media coverage almost instantly made OpenAI's blog post and coverage of it a huge source of discussion for the AI research community.

## The Reactions

As is often the case, most reporting from reputable sources covered the details accurately. For example, from
[OpenAI’s new multitalented AI writes, translates, and slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2):

> “...But as is usually the case with technological developments, these advances could also lead to potential harms. In a world where information warfare is increasingly prevalent and where nations deploy bots on social media in attempts to sway elections and sow discord, the idea of AI programs that spout unceasing but cogent nonsense is unsettling.

> "For that reason, OpenAI is treading cautiously with the unveiling of GPT-2. Unlike most significant research milestones in AI, the lab won’t be sharing the dataset it used for training the algorithm or all of the code it runs on...  ” 

At the same time, most coverage went with eye-catching headlines that ranged from "[New AI fake text generator may be too dangerous to release, say creators](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction)" to "[Researchers, scared by their own work, hold back “deepfakes for text” AI](https://arstechnica.com/information-technology/2019/02/researchers-scared-by-their-own-work-hold-back-deepfakes-for-text-ai/)".

<figure>
  <img class="image" src="/content/briefs/images/gpt2/stories.png" alt="Headlines about GPT2."/>
  <figcaption>
    Screenshot by Delip Rao, from <a href="http://deliprao.com/archives/314">When OpenAI tried to build more than a Language Model</a>
  </figcaption>
</figure>



Hyped up results don’t make anybody happy. It hurts AI researchers by
promoting fear mongering or setting unrealistic expectations, and the public
by promoting incorrect understanding of their work. So finding
out about new results in their field through such headlines prompted many in the AI research community 
to voice criticism of OpenAI's arguably PR-first communication of research and the decision of not publicly
releasing their models. Some lauded the move as a bold and necessary step towards responsible AI:

<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">Going so far as to think ahead to malicious uses and check in with stakeholders sets a new bar for ethics in AI. Well played <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@openai</a>, <a href="https://twitter.com/jackclarkSF?ref_src=twsrc%5Etfw">@jackclarkSF</a>.</p>&mdash; Brandon Rohrer (@_brohrer_) <a href="https://twitter.com/_brohrer_/status/1096411822939344896?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;d like to weigh in on the <a href="https://twitter.com/hashtag/GPT2?src=hash&amp;ref_src=twsrc%5Etfw">#GPT2</a> discussion. The decision not to release the trained model was carefully considered and important for norm-forming. Serving the public good requires us to draw lines on release somewhere: better long before catastrophe than after.</p>&mdash; Joshua Achiam (@jachiam0) <a href="https://twitter.com/jachiam0/status/1097030712945831937?ref_src=twsrc%5Etfw">February 17, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

Others considered it either a futile attempt or a poor decision doing more harm than good:

* Anima Anandkumar, Director of AI NVIDIA
<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">What you are doing is opposite of open. It is unfortunate that you hype up +propagate fear + thwart reproducibility+scientific endeavor. There is active research from other groups in unsupervised language models. You hype it up like it has never been done before. <a href="https://twitter.com/jackclarkSF?ref_src=twsrc%5Etfw">@jackclarkSF</a></p>&mdash; Anima Anandkumar (@AnimaAnandkumar) <a href="https://twitter.com/AnimaAnandkumar/status/1096209990916833280?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Zachary Lipton, Professor CMU
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Perhaps what&#39;s *most remarkable* about the <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> controversy is how *unremarkable* the technology is. Despite their outsize attention &amp; budget, the research itself is perfectly ordinary—right in the main branch of deep learning NLP research <a href="https://t.co/bmMkkL3KKJ">https://t.co/bmMkkL3KKJ</a></p>&mdash; Zachary Lipton (@zacharylipton) <a href="https://twitter.com/zacharylipton/status/1097021572714504192?ref_src=twsrc%5Etfw">February 17, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Denny Britz, former Google-Brain researcher
<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">Also consider: OpenAI is worried about the model falling into the wrong hands. But instead of releasing code/paper for researchers and trying to keep it low-key they make a huge PR splash, telling every last spammer on the planet. The choices seem a little… incompatible?</p>&mdash; Denny Britz (@dennybritz) <a href="https://twitter.com/dennybritz/status/1096280850704916480?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Richard Socher, Principal Scientist, Salesforce Research
<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">But their perplexity on wikitext-103 is 0.8 lower than previous sota. So it&#39;s dangerous now. 🙃<br><br>In other news. Copy and pasting and Photoshop are existential threats to humanity.<br><br>PS: Love language models. Love multitask learning. Great work. Dislike the hype and fear mongering.</p>&mdash; Richard (@RichardSocher) <a href="https://twitter.com/RichardSocher/status/1096195833865789442?ref_src=twsrc%5Etfw">February 14, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Mark O Riedl, Professor at Georgia Tech
<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">It’s a win-win-win for OpenAI. (1) It’s good work. (2) Not releasing the model artificially (and unnecessarily) inflates this perspective while simultaneously prohibiting replication. (3) Get to talk about saving the world by ramping up the fear of AI.</p>&mdash; Mark O. Riedl (@mark_riedl) <a href="https://twitter.com/mark_riedl/status/1096129834927964160?ref_src=twsrc%5Etfw">February 14, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">Let’s also stop and reflect on how reporters knew about this work longer than other scientists. Some of them reached out to academic researchers for comment, but it’s a powerful way to take early control of the narrative surrounding the technology.</p>&mdash; Mark O. Riedl (@mark_riedl) <a href="https://twitter.com/mark_riedl/status/1096214448807858176?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Francis Chollet, author of Keras, researcher at Google
<figure>
<blockquote class="twitter-tweet" data-cards="hidden" data-lang="en"><p lang="en" dir="ltr">We all want safe, responsible AI research. The first step is not misrepresenting the significance of your results to the public, not obfuscating your methods, &amp; not spoon-feeding fear-mongering press releases to the media.<br><br>That&#39;s our 1st responsibility.<a href="https://t.co/tqX07LbRit">https://t.co/tqX07LbRit</a></p>&mdash; François Chollet (@fchollet) <a href="https://twitter.com/fchollet/status/1097574572361019394?ref_src=twsrc%5Etfw">February 18, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Matt Gardner, research scientist  at AllenAI
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Key word here: &quot;humbly&quot;. I do not see what <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> did as &quot;humble&quot; - going _first_ to a ton of news organizations with your findings as way to further scientific discourse is pretty much the opposite of that. That&#39;s the only thing I took issue with. The research is great.</p>&mdash; Matt Gardner (@nlpmattg) <a href="https://twitter.com/nlpmattg/status/1097198430047264768?ref_src=twsrc%5Etfw">February 17, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

OpenAI’s reasoning was also openly satirized by many in the AI community:

* Yoav Goldberg, Professor at Bar Ilan University and Research Director of the Israeli branch of the Allen Institute for Artificial Intelligence
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Just wanted to give you all a heads up, our lab found an amazing breakthrough in language understanding. but we also worry it may fall into the wrong hands. so we decided to scrap it and only publish the regular *ACL stuff instead. Big respect for the team for their great work.</p>&mdash; (((ل()(ل() &#39;yoav)))) (@yoavgo) <a href="https://twitter.com/yoavgo/status/1096471273050382337?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

* Yann LeCun, Chief AI Scientist at Facebook AI Research, Professor at NYU
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&lt;trolling-joking&gt;<br>Every new human can potentially be used to generate fake news, disseminate conspiracy theories, and influence people.<br>Should we stop making babies then?<br>&lt;/trolling-joking&gt;</p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1097878015667851266?ref_src=twsrc%5Etfw">February 19, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

During this, Jack Clark and Miles Brundage at OpenAI were active in trying to provide answers, share the thought process, and take feedback on how they can improve things in the future.

<figure>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">I think there&#39;s a lot of stuff that we&#39;re gonna learn from this as we figure out future plans - as discussed in blog &quot;this decision, as well as our discussion of it, is an experiment&quot;.  We&#39;re also really excited to get feedback here and languagequestions@openai.com</p>&mdash; Jack Clark (@jackclarkSF) <a href="https://twitter.com/jackclarkSF/status/1096283377290768384?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">All good points. I think going forward we will be more transparent/proactive in our outreach to the rest of the AI community on this, and weight that more. At the time, it seemed like we had a few objectives to satisfy, inc. raising awareness of this broader class of systems...</p>&mdash; Miles Brundage (@Miles_Brundage) <a href="https://twitter.com/Miles_Brundage/status/1097583082155139072?ref_src=twsrc%5Etfw">February 18, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

The conversation was prolonged enough for many follow up articles to be written going more into-depth on these topics:

* [Some thoughts on zero-day threats in AI, and OpenAI's GP2](https://www.fast.ai/2019/02/15/openai-gp2/)
* [Should I Open-Source My Model?](https://towardsdatascience.com/should-i-open-source-my-model-1c109188b164)
* [OpenAI Trains Language Model, Mass Hysteria Ensues](http://approximatelycorrect.com/2019/02/17/openai-trains-language-model-mass-hysteria-ensues/)
* [An open and shut case on OpenAI](https://anima-ai.org/2019/02/18/an-open-and-shut-case-on-openai/)
* [OpenAI’s GPT-2: the model, the hype, and the controversy](https://towardsdatascience.com/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8)
* [When OpenAI tried to build more than a Language Model](http://deliprao.com/archives/314)
* [OpenAI: Please Open Source Your Language Model](https://thegradient.pub/openai-please-open-source-your-language-model/)
* [OpenAI Shouldn’t Release Their Full Language Model](https://thegradient.pub/openai-shouldnt-release-their-full-language-model/)
* [Dissecting the Controversy around OpenAI’s New Language Model](https://twimlai.com/twiml-talk-234-dissecting-the-controversy-surrounding-openais-new-language-model/)
* [Humans Who Are Not Concentrating Are Not General Intelligences](https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/)
* [Who’s afraid of OpenAI’s big, bad text generator?](https://thenextweb.com/artificial-intelligence/2019/02/26/whos-afraid-of-openais-big-bad-text-generator/)
* [OpenAI’s Recent Announcement: What Went Wrong, and How It Could Be Better](https://www.eff.org/deeplinks/2019/03/openais-recent-announcement-what-went-wrong-and-how-it-could-be-better)

In summary, while most people agreed with the need for talking about the
possible implications of the technology AI researchers are building,
many criticized OpenAI’s tact for the following reasons:

-   Giving reporters early information and preferential access to cutting edge research shows a focus on PR over making research contributions. 

-   The malicious uses of GPT2 were merely hypothesized, and OpenAI did not even make it possible for researchers in the field
    to request access to the model or data. Such practices may lead to the rise of gatekeeping and disincentivize transparency, reproducibility, and inclusivity in
    machine learning research.

-   Given the existence of similar models and open source code, the choice to not release the model seems likely to impact researchers and individuals not interested in large-scale malicious use. 
    The cost for training the model was estimated to $43,000, an amount
    insignificant for malicious actors, and a number of [clones](https://www.reddit.com/r/MachineLearning/comments/arrn36/p_openwebtext_open_clone_of_the_gpt2_webtext/)
    of the dataset used have popped up since OpenAI's announcement as well. 

-   OpenAI did a poor job of acknowledging prior [considerations about dual use](https://twitter.com/mmitchell_ai/status/1097626427048964098)
    in this space. 

## Our Perspective

<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Today&#39;s meta-Twitter summary for machine learning:<br>None of us have any consensus on what we&#39;re doing when it comes to responsible disclosure, dual use, or how to interact with the media.<br>This should be concerning for us all, in and out of the field.</p>&mdash; Smerity (@Smerity) <a href="https://twitter.com/Smerity/status/1096247080941629441?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

There is not much left to be said on this story, and the series of tweets above as well as the articles [Who’s afraid of OpenAI’s big, bad text generator?](https://thenextweb.com/artificial-intelligence/2019/02/26/whos-afraid-of-openais-big-bad-text-generator/) and [OpenAI’s Recent Announcement: What Went Wrong, and How It Could Be Better](https://www.eff.org/deeplinks/2019/03/openais-recent-announcement-what-went-wrong-and-how-it-could-be-better) cover what really needs to be said well. Even if OpenAI's stated intentions were authentic (which is likely the case given the company's prior focus on dual-use and stated focus on promoting practices that prevent misuse of AI technologies), a better thought out approach to communicating their new research and hypothetical concerns about it was definitely possible and needed. As stated well in [Who’s afraid of OpenAI’s big, bad text generator?](https://thenextweb.com/artificial-intelligence/2019/02/26/whos-afraid-of-openais-big-bad-text-generator/):

> "The general public likely still believes OpenAI made a text generator so dangerous it couldn’t be released, because that’s what they saw when they scrolled through their news aggregator of choice. But it’s not true, there’s nothing definitively dangerous about this particular text generator. Just like Facebook never developed an AI so dangerous it had to be shut down after inventing its own language. The kernels of truth in these stories are far more interesting than the lies in the headlines about them – but sadly, nowhere near as exciting.

> The biggest problem here is that by virtue of on onslaught of misleading headlines, the general public’s perception of what AI can and cannot do is now even further skewed from reality. It’s too late for damage-control, though OpenAI did try to set the record straight.

> No amount of slow news reporting can entirely undo the damage that’s done when dozens of news outlets report that an AI system is “too dangerous,” when it’s clearly not the case. It hurts research, destroys media credibility, and distorts politicians’ views.

> To paraphrase Anima Anandkumar: I’m not worried about AI-generated fake news, I’m worried about fake news about AI."

## TLDR

GPT-2 is not known to be 'too dangerous to release' , even if it might be. 
Whatever the motives of OpenAI may have been, discussion of how to most responsibly share new technology with a potential for misuse is good --- and misleading articles calling largely incremental AI advances 'dangerous' are bad. Hopefully, any future 'experiments' from OpenAI will results in more of the former and less of the latter.
